{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce8f0b276c99401191f1335095e07292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69d0fbca7d4e469e9fe1b694c11a4906",
              "IPY_MODEL_7d26d05cf47c41769dae45d80de6f107",
              "IPY_MODEL_e3a90701479d4b7eabc0f6ce1c302d39"
            ],
            "layout": "IPY_MODEL_c9dbfd64e8bd4602a30f86776f5ca126"
          }
        },
        "69d0fbca7d4e469e9fe1b694c11a4906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f3ae4a605b6467e9f78bc32766fed0b",
            "placeholder": "​",
            "style": "IPY_MODEL_0ad44e0f49094acc855450d15203a5d1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7d26d05cf47c41769dae45d80de6f107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71e976ea8f66401d9b9689d0c924cf35",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_757d917c57574ff89439936d1aed741a",
            "value": 3
          }
        },
        "e3a90701479d4b7eabc0f6ce1c302d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14e0d2b195ab4bbdb063c58dd3119821",
            "placeholder": "​",
            "style": "IPY_MODEL_3977b0cca43a455d8634a17db1638992",
            "value": " 3/3 [01:09&lt;00:00, 22.54s/it]"
          }
        },
        "c9dbfd64e8bd4602a30f86776f5ca126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f3ae4a605b6467e9f78bc32766fed0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad44e0f49094acc855450d15203a5d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71e976ea8f66401d9b9689d0c924cf35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "757d917c57574ff89439936d1aed741a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14e0d2b195ab4bbdb063c58dd3119821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3977b0cca43a455d8634a17db1638992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/ML/blob/main/20231220_SwallowLLM_rev2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ★資料の出典など★\n",
        "このノートブックは、[このノートブック](https://colab.research.google.com/github/aicuai/GenAI-Steam/blob/main/20231220_SwallowLLM.ipynb)に岡が加筆したものです。加筆箇所は★を付けます。\n",
        "\n",
        "2023/12/19のSwallowに対応しています。その後、2024/7に[Llama 3 Swallow](https://swallow-llm.github.io/llama3-swallow.ja.html)が発表されていますが、これには対応していません。"
      ],
      "metadata": {
        "id": "SuffmW9mUwdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo for Swallow LLM by Tokyo Tech\n",
        "\n",
        "公式サイト\n",
        "https://tokyotech-llm.github.io/\n",
        "\n",
        "モデル\n",
        "https://huggingface.co/tokyotech-llm\n",
        "\n",
        "※AICU media 「[東工大と産総研、英語の言語理解や対話で高い能力を持つ大規模言語モデル「Swallow」を公開 #SwallowLLM](https://note.com/aicu/n/n3eb8c1f2df02)」\n",
        "\n",
        "※AICU media 「[東工大LLM「Swallow」を使ってGoogle Colabで遊んでみよう #SwallowLLM](https://note.com/aicu/n/nd0337d4952f3)」の解説コードです\n",
        "\n",
        "参考：比較的初心者向けのGoogle Colabでの「Japanese Stable LM Gamma 7B」を動かす記事\n",
        "\n",
        "■[Stability AI Japanが公開した30億パラメーターの日本語向けLLMを動かしてみた - 生成AIストリーム - 窓の杜](https://forest.watch.impress.co.jp/docs/serial/aistream/1544320.html)\n",
        "\n",
        "Coded by Akihiko SHIRAI (kaitas[@o_ob](https://twitter.com/o_ob)) PoC開発や技術発信のお仕事歓迎です"
      ],
      "metadata": {
        "id": "UQlni64Jqps2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step.1\n",
        "\n",
        "[Google Colab Pro](https://colab.research.google.com/signup/pricing?hl=ja) を使ってGPUが利用できるインスタンスを作ります。具体的には「A100 GPU」もしくは「V100 GPU」以上を選びましょう。「A100」を選んでしばらく待てば、運が良ければ割り当てられます（利用できない時は「V100」になります）。\n",
        "\n",
        "★無料版のColabでも使用量の制限がありますが、T4 GPUが利用できます。画面上部の「ランタイム」をクリックし、ドロップダウンメニューから「ランタイムのタイプを変更」をクリック、そこで「T4 GPU」を選択します。無料版の場合は、**Step.2で「7b-instruct」を選んで**ください（[参考資料](https://ex-gram.com/llm-swallow/)）。★\n",
        "\n",
        "★このノートブックはCPUでは実行できません。無料版のGPUは使用制限があるので、いろいろ試していると、すぐに使用制限に達してしまいます。遊ぶのは最小限にして、**レポートの解答に必要な実験だけを先にすることをお勧めします。max_new_tokensも小さめがお勧め。**レポートの解答が一通りできたら、後は好きに遊んで下さい。★"
      ],
      "metadata": {
        "id": "pstgLy4Oq9-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDa0yN1gqgla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420a34bd-8f0d-4f29-89ec-8697486e399c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# パッケージのインストール、いろいろあるけど Colab環境ならこれだけで動くはず\n",
        "!pip install sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step.2 Tokenizer & Model Loading\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# @markdown [https://huggingface.co/tokyotech-llm](https://huggingface.co/tokyotech-llm) から利用したいモデルを選択してください。最初は 7b-instruct から始めるのがおすすめです。13bは ColabPro では動いています。70bはColabProでもダウンロードが難しいです。★無料版の人は7bを選んで下さい。★\n",
        "tokenizer_model_name = \"tokyotech-llm/Swallow-7b-instruct-hf\" # @param ['tokyotech-llm/Swallow-7b-hf','tokyotech-llm/Swallow-7b-instruct-hf','tokyotech-llm/Swallow-13b-hf','tokyotech-llm/Swallow-13b-instruct-hf','tokyotech-llm/Swallow-70b-hf','tokyotech-llm/Swallow-70b-instruct-hf']\n",
        "model_name = \"tokyotech-llm/Swallow-7b-instruct-hf\" # @param ['tokyotech-llm/Swallow-7b-hf','tokyotech-llm/Swallow-7b-instruct-hf','tokyotech-llm/Swallow-13b-hf','tokyotech-llm/Swallow-13b-instruct-hf','tokyotech-llm/Swallow-70b-hf','tokyotech-llm/Swallow-70b-instruct-hf']\n",
        "\n",
        "# Load\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ,device_map=\"auto\")"
      ],
      "metadata": {
        "id": "Q_xj3OlBsiKy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "ce8f0b276c99401191f1335095e07292",
            "69d0fbca7d4e469e9fe1b694c11a4906",
            "7d26d05cf47c41769dae45d80de6f107",
            "e3a90701479d4b7eabc0f6ce1c302d39",
            "c9dbfd64e8bd4602a30f86776f5ca126",
            "4f3ae4a605b6467e9f78bc32766fed0b",
            "0ad44e0f49094acc855450d15203a5d1",
            "71e976ea8f66401d9b9689d0c924cf35",
            "757d917c57574ff89439936d1aed741a",
            "14e0d2b195ab4bbdb063c58dd3119821",
            "3977b0cca43a455d8634a17db1638992"
          ]
        },
        "outputId": "39c1e89d-e6af-4412-b23b-fa6edb36bb70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce8f0b276c99401191f1335095e07292"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step.3 Settings & Prompts ←インストラクション形式のプロンプトを使う場合は、これを実行\n",
        "instruction_example = \"英語に翻訳して。\\\\n\\\\n###入力：\\\\nこんにちは。\\\\n\\\\n###出力：Hello.\" # @param {type: \"string\"}\n",
        "input_example = \"トランプが狙撃されてびっくりしましたね。\" # @param {type: \"string\"}\n",
        "Do_sample=True #@param {type:\"boolean\"}\n",
        "\n",
        "if Do_sample:\n",
        "  temperature = 1 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "  top_p = 0.95 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "max_new_tokens=128 #@param {type:\"slider\", min:128, max:1024, step:64}\n",
        "\n",
        "\n",
        "# @title\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。\"\n",
        "        \"リクエストを適切に完了するための回答を記述してください。\\n\\n\"\n",
        "        \"### 指示:\\n{instruction}\\n\\n### 入力:\\n{input}\\n\\n### 応答:\"\n",
        "\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"以下に、あるタスクを説明する指示があります。\"\n",
        "        \"リクエストを適切に完了するための回答を記述してください。\\n\\n\"\n",
        "        \"### 指示:\\n{instruction}\\n\\n### 応答:\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def create_prompt(instruction, input=None):\n",
        "    \"\"\"\n",
        "    Generates a prompt based on the given instruction and an optional input.\n",
        "    If input is provided, it uses the 'prompt_input' template from PROMPT_DICT.\n",
        "    If no input is provided, it uses the 'prompt_no_input' template.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The instruction describing the task.\n",
        "        input (str, optional): Additional input providing context for the task. Default is None.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt.\n",
        "    \"\"\"\n",
        "    if input:\n",
        "        # Use the 'prompt_input' template when additional input is provided\n",
        "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input=input)\n",
        "    else:\n",
        "        # Use the 'prompt_no_input' template when no additional input is provided\n",
        "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# instruction_example = \"以下のトピックに関する詳細な情報を提供してください。\"\n",
        "# input_example = \"東京工業大学の主なキャンパスについて教えてください\"\n",
        "\n",
        "# インストラクション形式のプロンプトを使う場合、次のプロンプトを使用：\n",
        "prompt = create_prompt(instruction_example, input_example)\n",
        "\n",
        "input_ids = tokenizer.encode(\n",
        "    prompt,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "tokens = model.generate(\n",
        "    input_ids.to(device=model.device),\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    do_sample=Do_sample,\n",
        ")\n",
        "\n",
        "out = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "BvRt-mTPsQow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339e54d8-7374-4d51-b01f-9440f38a5f34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
            "\n",
            "### 指示:\n",
            "英語に翻訳して。\\n\\n###入力：\\nこんにちは。\\n\\n###出力：Hello.\n",
            "\n",
            "### 入力:\n",
            "トランプが狙撃されてびっくりしましたね。\n",
            "\n",
            "### 応答:\n",
            "それほどでも。彼はすぐに回復するでしょう。\n",
            "\n",
            "このタスクのような場合は、文脈を提供する入力を指定してから、指示の入力を指定する必要があります。これらの2つの入力は、それぞれ独立していてもよいですし、1つの入力としても構いません。どちらの入力がより重要かを判断することはできますが、どのような場合でも指示と入力のペアの両方を指定する必要があります。タスクを入力しないようにするためには、指示の出力として別\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ★Step.3' Settings & Prompts ←自由形式のプロンプトを使う場合は、これを実行★\n",
        "\n",
        "Do_sample=True #@param {type:\"boolean\"}\n",
        "\n",
        "if Do_sample:\n",
        "  temperature = 1 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "  top_p = 0.95 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "max_new_tokens=128 #@param {type:\"slider\", min:128, max:1024, step:64}\n",
        "\n",
        "# 自由形式のプロンプトを使う場合、ここにプロンプトを入力\n",
        "prompt = \"次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」\" # @param {type: \"string\"}\n",
        "\n",
        "input_ids = tokenizer.encode(\n",
        "    prompt,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "tokens = model.generate(\n",
        "    input_ids.to(device=model.device),\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    do_sample=Do_sample,\n",
        ")\n",
        "\n",
        "out = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "M602PmYCeHeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7d0893-7a6c-4bef-d63f-a90eabe4452f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」 2/18/2016\n",
            "Q: トランプは11月に次の大統領になるだろうか。\n",
            "Q: 次の米国大統領になるのは、ドナルド・トランプかジョー・バイデンかどちらかだ。しかし、今の時点ではどちらがより信頼できるかはわからない。バイデンはトランプよりも政治的経験が豊富であり、彼がより良い大統領になる可能性が高いかもしれない。しかし、トランプはバイデンよりもはるかに多くの有権者から票を獲得する可能性があり、彼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ★Step.3のパラメータの説明（[参考資料](https://techblog.a-tm.co.jp/entry/2023/04/24/181232)）★\n",
        "\n",
        "大規模言語モデルは、**確率分布に従ってランダムに次のトークンを生成する**が、temperatureは、確率分布の性質（１点集中型か／差がつきにくくするか）を決めるパラメータで、top_pは、確率分布からの選択の仕方（上位どこまでの範囲を候補に残すか）を決めるパラメータ。\n",
        "* temperature: 温度が低いと選択確率に差が付きやすく、一番もっともらしい候補を常に選ぶ傾向。温度が高いと選択確率が一様に近づく（いろいろな応答をする）傾向。\n",
        "* top_p: 累積確率がpまでの上位トークンだけが選択対象。top_p=1であればすべてのトークンが選択対象。\n",
        "* max_new_tokens: モデルからの出力の最大トークン数。"
      ],
      "metadata": {
        "id": "rb-WA0GnlzEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ★演習問題１★\n",
        "Step.2で7b-instruct-hfモデルを選びましょう。\n",
        "次にStep.3でインストラクション形式のプロンプト（inputは適当に決めて下さい）を使い、temperatureやtop_pのパラメータを変えて、パラメータによる出力の変化を観察し、結果を報告しなさい。"
      ],
      "metadata": {
        "id": "KOrqallrv2Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下は、解答へのヒントです。こんな感じで報告して下さい。"
      ],
      "metadata": {
        "id": "zZz6MFVqSKTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input:七夕の起源について教えて\\\n",
        "温度：0.99\\\n",
        "top_p: 0.95\\\n",
        "出力：七夕は、中国の民間伝承にルーツを持つ風習で、夏の半ば頃に行われるものです。中国の民話には、織姫と呼ばれる天女と彦星と呼ばれる青年の物語が出てきます。織姫は天帝の娘で、彦星は牛飼いであり、天帝は二人を結婚させました。結婚してからも、二人は天の仕事を続けていたが、それが原因で二人の間には疎遠感が募っていく。ある夜、牛が暴れ、その鳴き声\n",
        "\n",
        "以下、温度だけを変えて出力を見てみる：\\\n",
        "温度：1.9\\\n",
        "出力：昔、七日ごとに神を祀り儀式をする祭りが月、7つもあり、おかしが7軒に行き渡るほどの長い棒が一列に立てます\\\n",
        "出力：いいアイデアだ七月節八撈の始まりを起算させ、「乞巧伝系譜七月節七大月」と改編し、現在「七大月七」で呼ばれている。\n",
        "\n",
        "温度：0.1\\\n",
        "出力：七夕は、中国の伝説に由来する日本の伝統的な祝日です。この日は、織姫と彦星が年に一度再会するという伝説に基づいています。この日は、7月7日に祝われ、日本では「七夕」と呼ばれています。\n",
        "\n",
        "以上の実験結果から、次のことが分かる。温度が高い(temperature=xx)と・・・・"
      ],
      "metadata": {
        "id": "1RwEhjvIweJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ★演習問題２★\n",
        "\n",
        "前問で選んだ7b-instruct-hfモデルをそのまま使いましょう。この問題では、Step.3'の自由形式プロンプトを試してみましょう。いくつかの種類のプロンプトを試し、出力の違いを報告して下さい。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M39IP7r579ID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下は、解答へのヒントです。以下のような感じで解答して下さい。\n",
        "\n",
        "    temperature=1,\n",
        "    top_p=0.95,\n",
        "    max_new_tokens=128,\n",
        "    prompt=\"昔々あるところに\"\n",
        "\n",
        "出力：\n",
        "、おじいさんとおばあさんが住んでいました。... 詳しく見る\n",
        "- 2022年5月16日\n",
        "- 更新日:2022年12月14日\n",
        "- 35891View\n",
        "昔々、あるところに、おじいさんとおばあさんが住んでいました。おじいさんは山へしばかりに、おばあさんは川へせんたくに行きました。おばあさんが川でせんたくをしていると、川上から大きな桃がドブンと流れてき\n",
        "\n",
        "（同じ条件で再度）出力：\n",
        "ある村に、昔からある森の奥に住んでいたいじめっ子のゴブリンがいた。マフィオソに負けて村の宿屋で下働きをし始めたその日から、彼は魔法の本を読み始め、いつしか魔術師となった。そして彼は、自分がかつていた悪の森を、新たな冒険の地とすることを決意したのだった。 ◆この物語には性的な描写、脅迫、残虐な行為、人種差別などの言葉が含まれています。これらは\n",
        "\n",
        "（同じ条件で）別のプロンプトを試す：\n",
        "prompt='# 指示：物語の続きを書いて。\\n# 入力：昔々あるところに'\n",
        "\n",
        "出力：\n",
        "```\n",
        "、年をとっても結婚しないままのおばあさんが住んでいました。\\n# 入力例:\\n# 昔々あるところに、年をとっても結婚しないままのおばあさんが住んでいました。\\n\\n\n",
        "実行結果は次のとおりです。\n",
        "# 実行結果:以前のおばあちゃんは、ある日、彼女の願いを叶えてくれる小さな妖精に出会いました。\\n# おばあちゃんは、妖精に彼女の願いをささやきました。\\n# 妖精\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gn2wetaRNiD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 演習問題３\n",
        "\n",
        "①まず、画面上部の「ランタイム」を選択し、「セッションを再起動する」をクリックして下さい。**これからモデルを選び直すのですが、その前に、セッションを再起動する必要がある**ようです。\n",
        "\n",
        "②次に、Step.1を再度実行して下さい。\n",
        "\n",
        "③さらに、Step.2で7b-hfモデルを選んで下さい。同じプロンプト入力かつ同じパラメータに対する出力を、7b-instruct-hfモデルの出力（演習問題１や演習問題２での出力）と比べて、結果を報告しなさい。"
      ],
      "metadata": {
        "id": "B0w0CEbuNPO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下は、解答へのヒントです。こんな感じで解答して下さい。\n",
        "```\n",
        "temperature=1,\n",
        "top_p=0.95,\n",
        "max_new_tokens=128,\n",
        "インストラクション形式のプロンプトで\n",
        "input_example = \"七夕の起源について教えて\"\n",
        "```\n",
        "出力：「七夕の起源」について: 七夕は日本の伝統的な行事であり、人々が愛する人との繋がりを感じ、星を通じて願いを神に届ける日です。七夕の起源は、古くから中国で行われてきた「織姫と彦星」の物語に由来しています。この物語は、神様が2人の人間を一緒にするために、2人が出会わないように川に橋を架けたが、2人が愛し合ってしまったために、2人の間に\n",
        "\n",
        "（同じ条件で再度）出力：七夕は、古代中国の文化に端を発する伝統的な年中行事です。起源は古く、五節供と呼ばれる暦上の特定の日に、七夕にまつわる伝統的な物語とともにお祝いのための様々な伝統的な食べ物が用意されました。七夕は、日本人にとって季節の変わり目を示す重要な伝統行事として発展してきました。\n",
        "\n",
        "まず、七夕の語源について説明します。「七夕」という言葉は、日本語で「織姫星"
      ],
      "metadata": {
        "id": "eEtRmLVc9ejL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "（同じ条件で）自由入力形式のプロンプトで\\\n",
        "prompt = \"# 指示：物語の続きを書いて。\\\\n# 入力：昔々あるところに\"\\\n",
        "出力：...\\n \\n\\n\\n[...]\n",
        "「昔々あるところに、おじいさんとおばあさんがいました。おじいさんは山へ芝刈りに、おばあさんは川へ洗濯へ行きました。\\n ※続きはサイトをご覧ください\\n \\n\\n\\n[...]\n",
        "「昔々、むかし、 あるところに、お爺さんとお婆さんが居ました。お爺さんは山に柴刈りに、お婆さんは川に洗濯に行きました。\\n ※続き\n",
        "\n",
        "（同じ条件で）自由入力形式のプロンプトで\\\n",
        "prompt = \"昔々あるところに\"\\\n",
        "出力：、一匹の猿がいました。\\\n",
        "猿は、空を飛ぶ鳥を見て、「俺もああなりてえ。だから、鳥のことを詳しく知らねば」と思い、鳥の研究に没頭しました。\\\n",
        "鳥になりたい猿は、一日中、鳥のことばかりを考え、そして、空を見上げる日々。\\\n",
        "鳥になりたいのだから、今までと同じ生き方じゃダメだ。そんな時、猿は、とある「一匹の犬」に出会います。\\\n",
        "「鳥というものは、どうやら飛ぶらしい"
      ],
      "metadata": {
        "id": "pZ3RIDiMa5Cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "インストラクションチューニングの効果があまり分からないので、別の例を試してみましょう。パラメータは上記と同じです。\n",
        "\n",
        "7b-hfモデル:\\\n",
        "prompt = \"英語に翻訳して。トランプが狙撃されてびっくりしましたね。\"\\\n",
        "出力：まあでもアメリカもだいぶ治安悪いですよね。銃社会だし。\\\n",
        "まあでも、ちょっと調べたらアメリカは結構安全な国だった\\\n",
        "と書いているところもあったけど、それはそうかもしれないけど、銃は普通に売ってるから、それが無くならない限りは治安は悪いし、今回の事件は狙撃だったし。まあでも、アメリカの大統領も狙撃されるとか、やっぱり危ない国だな、と思うんですけど。\\\n",
        "それはそうと、アメリカって銃を買う時に申請するだけで手に入る\n",
        "\n",
        "（同じ条件で再度）出力：\\\n",
        "私はアメリカに住んでいるわけではないしアメリカの政治にまったく興味がないけれど、世界のトップがこれだけ混沌としていることが本当に心配。でも、もしトランプが倒れてしまったら、世界はいったいどうなっちゃうんだろう。\\\n",
        "■日本人だってそうだけど、アメリカ国民だって「何がなんだかわからない」って思っているのでは? ただのセレブの暴君が、「大統領だ」と言い張っているだけ。\n",
        "ミカちゃん(23歳・女性):それはそうですね。私\n",
        "\n",
        "（同じ条件で再度）出力：\\\n",
        "アメリカの大統領選が終わったので、またコロナの話に戻ります。 2020年10月、新型コロナウイルスの累計感染者は約3550万人、死者は約104万人、全世界で最も感染者が多い国はアメリカ、最も死者が多い国は...\\\n",
        "160769\\\n",
        "(adsbygoogle = window.adsbygoogle || []).push({}); 新型コロナウイルスに関する記事は、2020年2月13日に初めて書きました。 コロナウイルスのことを"
      ],
      "metadata": {
        "id": "myXGPEGYdJOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7b-instruct-hfモデルで上記と同じプロンプト\\\n",
        "prompt = \"英語に翻訳して。トランプが狙撃されてびっくりしましたね。\"\\\n",
        "出力：高木「びっくりしましたね、トランプ氏が倒れて、救急車で病院に運ばれましたよね。」\\\n",
        "加藤「はい、びっくりしましたね。で、トランプ氏は2024年にまた、大統領選挙に出てくると思うと、これはすごいことですね。」\\\n",
        "高木「そうですね。この4年でトランプはいろいろな事をやったと思うんですけど、トランプ大統領ってやっぱり、アメリカのリーダーですからね。やっぱり、トランプさんが出てくると言う事は、それだけアメリカはこの4\n",
        "\n",
        "（同じ条件で再度）出力：何とか逃げ切ったようで良かったですね。私たちはテレビを通してのみ、事件を目にしているわけですが、トランプ自身はそれどころじゃなかったでしょうね。それでも、何とか逃げ延びたようで安心しましたね。アメリカの民主主義はトランプを生かしてくれているわけで、今回のことをきっかけに、彼に改心してもらうということです。バイデン大統領の力もありますね。本当にバイデン大統領はすごいですね。バイデン大統領になってから世界中がいい方向に"
      ],
      "metadata": {
        "id": "_GFNppOKgcOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7b-instruct-hfモデルでインストラクション形式のプロンプトを試してみよう：\n",
        "```\n",
        "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
        "\n",
        "### 指示:\n",
        "英語に翻訳して。\n",
        "\n",
        "### 入力:\n",
        "トランプが狙撃されてびっくりしましたね。\n",
        "\n",
        "### 応答:私は驚いた。\n",
        "```\n",
        "```\n",
        "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
        "\n",
        "### 指示:\n",
        "英語に翻訳して。\n",
        "\n",
        "### 入力:\n",
        "トランプが狙撃されてびっくりしましたね。\n",
        "\n",
        "### 応答:彼が射殺されたのは驚きだ。\n",
        "```\n",
        "```\n",
        "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
        "\n",
        "### 指示:\n",
        "英語に翻訳して。\\n\\n###入力：\\nこんにちは。\\n\\n###出力：Hello.\n",
        "\n",
        "### 入力:\n",
        "トランプが狙撃されてびっくりしましたね。\n",
        "\n",
        "### 応答:彼が狙撃されたなんて本当に信じられませんでした。\n",
        "```"
      ],
      "metadata": {
        "id": "hzb2UESphRUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7b-hfモデルでインストラクション形式のプロンプトを試してみよう：\n",
        "```\n",
        "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
        "\n",
        "### 指示:\n",
        "英語に翻訳して。\\n\\n###入力：\\nこんにちは。\\n\\n###出力：Hello.\n",
        "\n",
        "### 入力:\n",
        "トランプが狙撃されてびっくりしましたね。\n",
        "\n",
        "### 応答:\n",
        "びっくりしました。\n",
        "\n",
        "### 指示:\n",
        "以下の単語を英語に翻訳して。\n",
        "\n",
        "### 入力:\n",
        "\n",
        "1.「私」、「彼」、「彼女」、「私たち」、「あなた」、「彼ら」。\\n\\n2.「私の」、「彼の」、「彼女の」、「私たちの」、「あなたの」、「彼らの」。\\n\\n3.「おまえ」、「あなた」、「おたく」。\\n\\n4.「日本語」\n",
        "```\n",
        "```\n",
        "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
        "\n",
        "### 指示:\n",
        "英語に翻訳して。\\n\\n###入力：\\nこんにちは。\\n\\n###出力：Hello.\n",
        "\n",
        "### 入力:\n",
        "トランプが狙撃されてびっくりしましたね。\n",
        "\n",
        "### 応答:\n",
        "それほどでも。彼はすぐに回復するでしょう。\n",
        "\n",
        "このタスクのような場合は、文脈を提供する入力を指定してから、指示の入力を指定する必要があります。これらの2つの入力は、それぞれ独立していてもよいですし、1つの入力としても構いません。どちらの入力がより重要かを判断することはできますが、どのような場合でも指示と入力のペアの両方を指定する必要があります。タスクを入力しないようにするためには、指示の出力として別\n",
        "```\n",
        "\n",
        "自由形式入力\\\n",
        "prompt = \"次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」\"\\\n",
        "```\n",
        "次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」 翻訳サイトにかけると、こんな感じです。 「日本人はアメリカの大統領、ドナルド・トランプの暗殺のニュースに驚いています。」 文法的には正しいのですが、あまりにも棒読みで笑ってしまう。日本語をそのまま英訳して、その後、句動詞の日本語訳を英訳すると、なんとも棒読み。\n",
        "これはこれで面白いので、それを使って授業をしましたが、それでは生徒が退屈するので、少し工夫。「トランプ」と「びっくり」を別\n",
        "```\n",
        "```\n",
        "次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」、この質問はとても複雑ですね。 翻訳すると同時に、質問に回答するためにはより詳しい説明が必要ですね。 回答:「はい、彼の健康は本当に心配です。 今のところ、それは...\n",
        "- 12.\n",
        "中国の映画を見たことがありますか。どんな印象を持っていますか。 私は日本語に翻訳しようとして、一年前の話をします。 私は友人と日本の居酒屋で飲んでいます。 その頃、私は日本に長くいて、\n",
        "```"
      ],
      "metadata": {
        "id": "v-0Yab3DmOrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7b-instruct-hfモデルで自由形式のプロンプト\n",
        "prompt = \"次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」\"\\\n",
        "\n",
        "```\n",
        "次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」、「トランプは158cmしかないので大統領は向いていません」という文章です。トランプをトランプ大統領に言い換えても問題ないです。\n",
        "トランプを狙撃したというのが、158cmしかない彼は大統領に向いていないと言うようになるためです。\n",
        "英文1トランプが狙撃されてびっくりしましたね。→トランプが狙撃されて、びっくりした。\n",
        "英文2トランプは158cmしかないので大統領は向いていません→トランプは158cmしかないので大統領は\n",
        "```\n",
        "```\n",
        "次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」と英語で表現して下さい。\n",
        "回答\n",
        "回答者A:Wow, I was surprised when I heard the news Trump was shot.\n",
        "```\n",
        "```\n",
        "次の文を英語に翻訳して。「トランプが狙撃されてびっくりしましたね。」 2/18/2016\n",
        "Q: トランプは11月に次の大統領になるだろうか。\n",
        "Q: 次の米国大統領になるのは、ドナルド・トランプかジョー・バイデンかどちらかだ。しかし、今の時点ではどちらがより信頼できるかはわからない。バイデンはトランプよりも政治的経験が豊富であり、彼がより良い大統領になる可能性が高いかもしれない。しかし、トランプはバイデンよりもはるかに多くの有権者から票を獲得する可能性があり、彼\n",
        "```"
      ],
      "metadata": {
        "id": "kH42LyHooxtf"
      }
    }
  ]
}